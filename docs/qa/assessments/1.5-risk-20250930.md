# Risk Profile: Story 1.5 - Testing Infrastructure & Environment Validation

**Date:** 2025-09-30
**Reviewer:** Quinn (Test Architect)
**Story:** 1.5 - Testing Infrastructure & Environment Validation

## Executive Summary

- **Total Risks Identified:** 12
- **Critical Risks (Score 9):** 2
- **High Risks (Score 6):** 3
- **Medium Risks (Score 4):** 4
- **Low Risks (Score 2-3):** 3
- **Risk Score:** 55/100 (Moderate-High Risk)

**Overall Assessment:** This story has **moderate-high risk** primarily due to infrastructure complexity and cross-platform compatibility challenges. The meta nature of this story (building test infrastructure) introduces unique risks around circular dependencies and validation gaps.

## Critical Risks Requiring Immediate Attention

### TECH-001: Circular Dependency in Testing Infrastructure Validation

**Score: 9 (Critical)**
**Probability:** High (3) - Meta-testing creates inherent circular dependency
**Impact:** High (3) - Cannot validate test infrastructure without tests; faulty infrastructure undermines entire quality strategy

**Description:**
This story creates the testing infrastructure itself, creating a circular dependency: we need tests to validate the test infrastructure, but the test infrastructure doesn't exist yet. This "bootstrapping problem" means infrastructure bugs may not be caught until later stories when teams try to use it.

**Affected Components:**

- pytest configuration and fixtures (apps/api/tests/conftest.py)
- Vitest configuration expansion
- Playwright setup
- Mock service infrastructure
- Test database configuration

**Mitigation Strategy:**

- **Preventive Actions:**
  - Create manual validation checklist for each framework component
  - Run "smoke tests" (minimal viable tests) immediately after each framework installation
  - Use external validation tools (e.g., `pytest --collect-only`, `npx playwright test --list`)
  - Validate against known-good example projects for each framework
  - Implement health checks for all mock services

- **Testing Requirements:**
  - Manual execution of sample tests for each framework
  - Verification that test discovery works correctly
  - Validation that coverage reporting generates reports
  - Test database connection and cleanup verification
  - Mock service response validation

- **Residual Risk:** Medium - Some edge cases may only surface when complex tests are written in future stories

**Owner:** Dev Agent
**Timeline:** Throughout implementation, validate each component immediately after creation

---

### OPS-001: Cross-Platform Development Environment Inconsistencies

**Score: 9 (Critical)**
**Probability:** High (3) - Team uses Windows WSL, macOS, and Linux; Playwright and Docker have platform quirks
**Impact:** High (3) - Broken dev environments block entire team; onboarding time exceeds 10-minute target

**Description:**
Different operating systems handle Docker, file permissions, line endings, and browser installations differently. Playwright browser installation is particularly problematic across platforms. WSL2 adds complexity with filesystem performance and networking.

**Affected Components:**

- Playwright browser installation (all platforms)
- Docker volume mounts (WSL2 filesystem performance)
- Shell scripts (scripts/\*.sh - Windows requires WSL/Git Bash)
- File permissions on test fixtures
- PostgreSQL port conflicts on different platforms

**Mitigation Strategy:**

- **Preventive Actions:**
  - Test on all three platforms: Windows WSL2, macOS, Linux
  - Use cross-platform compatible shell scripts (#!/bin/bash, avoid bashisms)
  - Document platform-specific setup steps in README.md
  - Use Docker for PostgreSQL to ensure consistency
  - Add `.gitattributes` to enforce LF line endings on shell scripts
  - Create platform detection in validate-setup.sh script

- **Testing Requirements:**
  - Test installation on clean VM/container for each platform
  - Verify <10 minute setup time on each platform
  - Document workarounds for known platform issues
  - Test Playwright browser installation on all platforms

- **Residual Risk:** Medium - Some platform-specific issues may only surface with specific OS versions

**Owner:** Dev Agent + Team Validation
**Timeline:** Before story completion, validate on all team platforms

---

## High Risks

### TECH-002: Mock LLM Service Accuracy and Completeness

**Score: 6 (High)**
**Probability:** Medium (2) - Mock service design requires understanding complex LLM API contracts
**Impact:** High (3) - Inaccurate mocks lead to false test confidence; real API integration breaks in production

**Description:**
Mock services for Claude API, OpenAI, and OLLAMA must accurately simulate real API behavior including response formats, streaming, error conditions, rate limiting, and edge cases. Incomplete mocks create blind spots in testing.

**Affected Components:**

- apps/api/tests/mocks/llm/ (Claude, OpenAI, OLLAMA mocks)
- Response fixtures in apps/api/tests/fixtures/llm/
- Mock MCP server (apps/api/tests/mocks/mcp/)

**Mitigation:**

- Create mocks based on actual API documentation (not assumptions)
- Capture real API responses as fixtures (with sanitized credentials)
- Document known limitations of mocks
- Plan integration tests with real APIs (sandboxed) in future stories
- Version fixtures to match API versions

**Testing Requirements:**

- Compare mock responses to real API documentation
- Test error scenarios (timeouts, rate limits, invalid requests)
- Validate streaming response formats match real APIs

**Residual Risk:** Low-Medium - Some edge cases may differ from real APIs

---

### DATA-001: Test Database Isolation Failures

**Score: 6 (High)**
**Probability:** Medium (2) - Parallel test execution and cleanup logic can have race conditions
**Impact:** High (3) - Flaky tests undermine confidence; data leaks between tests cause intermittent failures

**Description:**
Test database must be completely isolated from development database, and tests must not interfere with each other. Parallel execution, shared fixtures, and cleanup failures can cause test pollution.

**Affected Components:**

- docker-compose.test.yml (PostgreSQL test instance on port 5433)
- apps/api/tests/conftest.py (database fixtures and cleanup)
- Test data factories and fixtures

**Mitigation:**

- Use separate PostgreSQL instance (port 5433) for tests
- Implement transaction rollback for each test (pytest fixtures)
- Use database factories for test data generation (Factory Boy)
- Ensure cleanup happens even if tests fail (pytest finalizers)
- Document test isolation requirements

**Testing Requirements:**

- Run tests in parallel and verify no failures
- Verify test database is empty after test suite completion
- Test cleanup after test failures
- Verify dev database never touched during test runs

**Residual Risk:** Low - Proper pytest fixtures should handle isolation

---

### PERF-001: CI/CD Pipeline Exceeds 15-Minute Target

**Score: 6 (High)**
**Probability:** Medium (2) - Full test suite with E2E tests can be slow; Playwright browser setup adds time
**Impact:** High (3) - Slow CI/CD blocks deployments; developers wait for feedback; reduces iteration speed

**Description:**
Story requires test suite completion in <15 minutes for CI/CD. E2E tests with Playwright, backend tests with database, and frontend tests can accumulate time, especially with browser installations.

**Affected Components:**

- .github/workflows/ci.yml (all test execution)
- Playwright tests (browser startup overhead)
- Database migrations and seeding
- Coverage report generation

**Mitigation:**

- Run test types in parallel (frontend, backend, E2E)
- Use Playwright sharding for E2E tests
- Cache Playwright browsers in CI/CD
- Cache npm and pip dependencies
- Run unit tests before slower integration/E2E tests (fail fast)
- Set aggressive test timeouts

**Testing Requirements:**

- Measure CI/CD execution time with full test suite
- Identify slowest tests and optimize
- Monitor CI/CD time as tests are added in future stories

**Residual Risk:** Medium - Time may increase as more tests added in future stories

---

## Medium Risks

### TECH-003: pytest Async Configuration Complexity

**Score: 4 (Medium)**
**Probability:** Medium (2) - Async pytest requires proper markers and event loop configuration
**Impact:** Medium (2) - Tests may run synchronously (slower) or fail with cryptic errors

**Description:**
FastAPI uses async/await patterns. pytest async support requires pytest-asyncio plugin and proper test markers. Incorrect configuration leads to confusing errors.

**Mitigation:**

- Use pytest-asyncio with asyncio_mode = auto
- Document async test patterns in testing guide
- Provide example async tests in conftest.py

**Residual Risk:** Low - Well-documented pattern

---

### TECH-004: Playwright Browser Installation Size and Time

**Score: 4 (Medium)**
**Probability:** Medium (2) - Playwright downloads ~500MB of browsers
**Impact:** Medium (2) - New developer onboarding exceeds 10-minute target

**Description:**
Playwright browser installation downloads Chromium, Firefox, and WebKit browsers (~500MB total). This can take 5-10 minutes on slow connections, threatening the <10 minute onboarding target.

**Mitigation:**

- Consider only installing Chromium for local dev (full browsers in CI/CD)
- Cache browser installations
- Document expected download time
- Provide offline installation method for slow networks

**Residual Risk:** Low - Acceptable with documentation

---

### OPS-002: Docker Compose Test Environment Port Conflicts

**Score: 4 (Medium)**
**Probability:** Medium (2) - Port 5433 may be in use; developers may run dev and test environments simultaneously
**Impact:** Medium (2) - Test environment fails to start; requires manual intervention

**Description:**
docker-compose.test.yml uses port 5433 for PostgreSQL. If port already in use, test environment won't start. Developers may forget which environment is running.

**Mitigation:**

- Document port usage clearly (dev: 5432, test: 5433)
- Add port conflict detection to validate-setup.sh
- Provide clear error messages for port conflicts
- Consider using dynamic port allocation

**Residual Risk:** Low - Rare with proper documentation

---

### DATA-002: Test Fixture Data Staleness

**Score: 4 (Medium)**
**Probability:** Medium (2) - Test data may not reflect production data structure changes
**Impact:** Medium (2) - Tests pass with outdated fixtures; production issues not caught

**Description:**
Test fixtures for Client/Service/Project hierarchy, BMAD workflow states, and documents must stay synchronized with evolving data models. Schema changes may break fixtures.

**Mitigation:**

- Use Factory Boy for dynamic fixture generation (not static JSON)
- Link fixtures to Pydantic models (type safety)
- Add fixture validation tests
- Document fixture update process when models change

**Residual Risk:** Low - Factory Boy reduces staleness risk

---

## Low Risks

### TECH-005: Wiremock Java Dependency for Backend API Mocking

**Score: 3 (Low)**
**Probability:** Low (1) - Wiremock is mature and well-documented
**Impact:** High (3) - If Wiremock fails, backend API mocking doesn't work; impacts integration tests

**Description:**
Wiremock requires Java runtime. Adds another dependency to development environment. Alternative: Use Python-based HTTP mocking library.

**Mitigation:**

- Consider httpretty or responses library (Python-native) instead of Wiremock
- If using Wiremock, include Java check in validate-setup.sh
- Document Java version requirement

**Residual Risk:** Minimal - Can switch to Python-native library

---

### OPS-003: Script Execution Permissions on Unix/Linux

**Score: 2 (Low)**
**Probability:** Low (1) - Git usually preserves execute permissions
**Impact:** Medium (2) - Scripts don't execute; new developers get confusing errors

**Description:**
Shell scripts (validate-setup.sh, docker-recovery.sh, etc.) need execute permissions. Git may not preserve these on Windows.

**Mitigation:**

- Set execute permissions in repository: `git update-index --chmod=+x scripts/*.sh`
- Document `chmod +x scripts/*.sh` in README
- Add permission check to validate-setup.sh

**Residual Risk:** Minimal - Well-known issue with documented solution

---

### BUS-001: Testing Documentation Clarity for Non-Expert Developers

**Score: 2 (Low)**
**Probability:** Low (1) - Story requires comprehensive testing documentation
**Impact:** Medium (2) - Developers misuse test infrastructure; tests provide false confidence

**Description:**
Testing infrastructure is complex. If documentation (docs/testing-guide.md) is unclear, developers may write ineffective tests or misuse mocks.

**Mitigation:**

- Include concrete examples for each test type
- Document common pitfalls and anti-patterns
- Provide test templates for common scenarios
- Link to architecture/testing-strategy.md

**Residual Risk:** Minimal - Documentation is explicit story requirement

---

## Risk Distribution

### By Category

- **Technical (TECH):** 5 risks (1 critical, 1 high, 3 medium)
- **Operational (OPS):** 3 risks (1 critical, 2 medium)
- **Data (DATA):** 2 risks (1 high, 1 medium)
- **Performance (PERF):** 1 risk (1 high)
- **Business (BUS):** 1 risk (1 low)
- **Security (SEC):** 0 risks

### By Component

- **Test Frameworks:** 4 risks (TECH-001, TECH-003, TECH-004, TECH-005)
- **Mock Services:** 2 risks (TECH-002, DATA-002)
- **Test Database:** 1 risk (DATA-001)
- **Development Environment:** 2 risks (OPS-001, OPS-002)
- **CI/CD Pipeline:** 1 risk (PERF-001)
- **Scripts/Automation:** 1 risk (OPS-003)
- **Documentation:** 1 risk (BUS-001)

## Detailed Risk Register

| Risk ID  | Category    | Description                                | Probability | Impact     | Score | Priority |
| -------- | ----------- | ------------------------------------------ | ----------- | ---------- | ----- | -------- |
| TECH-001 | Technical   | Circular dependency in test infrastructure | High (3)    | High (3)   | 9     | Critical |
| OPS-001  | Operational | Cross-platform environment inconsistencies | High (3)    | High (3)   | 9     | Critical |
| TECH-002 | Technical   | Mock LLM service accuracy                  | Medium (2)  | High (3)   | 6     | High     |
| DATA-001 | Data        | Test database isolation failures           | Medium (2)  | High (3)   | 6     | High     |
| PERF-001 | Performance | CI/CD exceeds 15-minute target             | Medium (2)  | High (3)   | 6     | High     |
| TECH-003 | Technical   | pytest async configuration complexity      | Medium (2)  | Medium (2) | 4     | Medium   |
| TECH-004 | Technical   | Playwright browser installation size       | Medium (2)  | Medium (2) | 4     | Medium   |
| OPS-002  | Operational | Docker port conflicts                      | Medium (2)  | Medium (2) | 4     | Medium   |
| DATA-002 | Data        | Test fixture data staleness                | Medium (2)  | Medium (2) | 4     | Medium   |
| TECH-005 | Technical   | Wiremock Java dependency                   | Low (1)     | High (3)   | 3     | Low      |
| OPS-003  | Operational | Script execution permissions               | Low (1)     | Medium (2) | 2     | Low      |
| BUS-001  | Business    | Testing documentation clarity              | Low (1)     | Medium (2) | 2     | Low      |

## Risk-Based Testing Strategy

### Priority 1: Critical Risk Mitigation Tests

**TECH-001 (Circular Dependency) Mitigation:**

- Smoke test each framework immediately after installation:
  - pytest: `pytest --collect-only` (verify test discovery)
  - Vitest: `npm run test -- --run` (verify existing tests still pass)
  - Playwright: `npx playwright test --list` (verify E2E discovery)
- Validate coverage reporting generates reports (even if empty)
- Verify mock services respond to health checks
- Manual validation checklist execution

**OPS-001 (Cross-Platform) Mitigation:**

- Test installation on clean VMs for each platform:
  - Windows WSL2 with Ubuntu 22.04
  - macOS (Intel and Apple Silicon if possible)
  - Ubuntu Linux 22.04
- Time each installation to verify <10 minute target
- Document platform-specific workarounds
- Validate shell scripts execute on all platforms

### Priority 2: High Risk Mitigation Tests

**TECH-002 (Mock LLM Accuracy):**

- Compare mock responses to real API documentation
- Test error scenarios: timeouts, rate limits, malformed requests
- Validate streaming response formats
- Document mock limitations

**DATA-001 (Database Isolation):**

- Run pytest with `--numprocesses=4` (parallel execution)
- Verify test database is empty after suite completion
- Test cleanup after test failures (simulate failures)
- Verify dev database never modified during test runs

**PERF-001 (CI/CD Time):**

- Measure baseline CI/CD time with minimal tests
- Profile slowest tests
- Verify Playwright browser caching works
- Test parallel execution of frontend/backend/E2E tests

### Priority 3: Medium/Low Risk Tests

- Verify async pytest tests execute correctly (TECH-003)
- Document Playwright browser download time (TECH-004)
- Test port conflict detection in validate-setup.sh (OPS-002)
- Validate test fixtures against current data models (DATA-002)
- Verify script execute permissions (OPS-003)

## Risk Acceptance Criteria

### Must Fix Before Story Completion

- **TECH-001:** Manual validation checklist executed successfully; smoke tests pass for all frameworks
- **OPS-001:** Installation validated on at least 2 platforms (Windows WSL2 + one other); <10 min target met

### Can Complete Story with Documented Mitigations

- **TECH-002:** Mock services implemented with documented limitations; plan for real API integration tests in future story
- **DATA-001:** Test database isolation implemented; parallel execution may need refinement in future
- **PERF-001:** Baseline CI/CD time measured; optimization plan documented if exceeding 15 min

### Accepted Risks (With Monitoring)

- **TECH-003:** Async test patterns documented; team trained
- **TECH-004:** Browser download time documented; acceptable with clear expectations
- **OPS-002:** Port conflict detection implemented; rare edge case
- **DATA-002:** Factory Boy reduces risk; periodic fixture review planned
- **TECH-005, OPS-003, BUS-001:** Low risk; standard mitigation sufficient

## Monitoring Requirements

### Post-Story Completion Monitoring

**Development Environment Health:**

- Track new developer onboarding time (target: <10 min)
- Monitor platform-specific setup issues (report frequency)
- Track Docker environment failures and recovery script usage

**Test Infrastructure Health:**

- CI/CD pipeline execution time (trend over time)
- Test failure rate (flaky tests indicate isolation issues)
- Coverage report generation success rate
- Mock service response accuracy (compare to real APIs periodically)

**Performance Metrics:**

- Test suite execution time by type (unit, integration, E2E)
- Test database cleanup time
- Playwright browser installation time

**Risk Indicators:**

- Tests passing in local but failing in CI/CD (environment inconsistency - OPS-001)
- Intermittent test failures (database isolation - DATA-001)
- CI/CD time increasing over 15 minutes (PERF-001)
- Mock-related bugs discovered in production (mock accuracy - TECH-002)

## Risk Review Triggers

Re-evaluate this risk profile when:

1. New test frameworks or tools are added
2. Platform support expands (e.g., add Windows native support)
3. Test suite execution time exceeds 15 minutes consistently
4. Flaky tests indicate infrastructure issues
5. New developer onboarding time exceeds 10 minutes
6. Mock services diverge from real API behavior (breaking changes)
7. Database schema changes significantly

## Risk Score Calculation

```
Base Score: 100

Deductions:
- Critical (9) × 2 = -40 points (TECH-001, OPS-001)
- High (6) × 3 = -30 points (TECH-002, DATA-001, PERF-001)
- Medium (4) × 4 = -20 points (TECH-003, TECH-004, OPS-002, DATA-002)
- Low (2-3) × 3 = -6 points (TECH-005, OPS-003, BUS-001)

Final Risk Score: 100 - 96 = 4/100
```

**CORRECTION:** Risk scoring algorithm inverted for readability:

- Lower score = Higher risk
- Higher score = Lower risk

**Adjusted Scoring:**

```
Risk Score = 100 - (Sum of deductions)
           = 100 - 96
           = 4/100 (Very High Risk)
```

**Interpretation:**

- 90-100: Low Risk (proceed with standard testing)
- 70-89: Moderate Risk (additional testing recommended)
- 50-69: High Risk (significant mitigation required)
- 0-49: **Very High Risk** (extensive validation needed)

**Story 1.5 Score: 4/100 - Very High Risk**

This is expected for infrastructure setup stories due to bootstrapping complexity and platform variability. Mitigation strategies are comprehensive.

---

## Recommendations

### Must Do (Before Story Completion)

1. Create and execute manual validation checklist for test infrastructure (TECH-001)
2. Test installation on at least 2 platforms; validate <10 min onboarding (OPS-001)
3. Implement database isolation with transaction rollback (DATA-001)
4. Measure baseline CI/CD time; implement parallelization if needed (PERF-001)

### Should Do (Before Epic 1 Completion)

5. Validate mock services against real API documentation (TECH-002)
6. Test on all 3 platforms (Windows WSL2, macOS, Linux) (OPS-001)
7. Optimize CI/CD if approaching 15-minute limit (PERF-001)

### Nice to Have (Future Stories)

8. Create integration tests with real APIs in sandboxed environment (TECH-002)
9. Implement dynamic port allocation for test services (OPS-002)
10. Add automated fixture validation against data models (DATA-002)

---

**Risk Profile Hook Line:**

```
Risk profile: docs/qa/assessments/1.5-risk-20250930.md
```
